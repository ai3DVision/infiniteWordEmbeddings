\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm,algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{bbm}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

\newcommand{\argmin}{\operatornamewithlimits{argmin}}


\title{Infinite Dimensional Word Embeddings}


\author{Eric T. Nalisnick \thanks{ Authors contributed equally.} \\
Department of Computer Science\\
University of California, Irvine\\
Irvine, CA 92697, USA \\
\texttt{enalisni@uci.edu} \\
\And
Sachin Ravi $^{*}$\\
Department of Computer Science \\
Princeton University \\
Princeton, NJ 08540, USA \\
\texttt{sachinr@cs.princeton.edu} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
We describe a method for learning word embeddings with stochastic dimensionality.  Our \textit{Infinite Skip-Gram} model (iSGM) defines an energy-based joint distribution over a word vector, a context vector, and their dimensionality.  By employing the same techniques used to make the \textit{Infinite Restricted Boltzmann Machine} \citep{cote2015infinite} tractable, the iSGM admits a normalized conditional distribution over vector dimensionality, leading to an elegant probabilistic mechanism for word sense induction.  We show qualitatively and quantitatively that the iSGM produces parameter-efficient representations that are robust to language's inherent ambiguity.  [State of the art performance on semantic similarity and word sense disambiguation tasks is reported.]  
\end{abstract}

\section{Introduction}
Neural word embedding (NWE) methods \citep{mnih2009scalable, turian2010word, mikolov2013distributed} have received wide-spread attention for their ability to capture surprisingly detailed semantic information without supervision.  These methods map each word to a point in high-dimensional, real-valued space--a so-called \textit{distributed representation}.  In the learnt space, neighboring words correspond to like entities, and vector offsets have been shown to encode geographic and grammatical relationships \citep{mikolov2013linguistic}.  However, despite their success, there are still deficiencies with NWEs.  One such flaw is that the vector representations--since their dimensionality is fixed and standard across the vocabulary--do not accurately reflect each word's semantic complexity.  For instance, the meaning and even part-of-speech (POS) of  the word \textit{drug} are context dependent (ex: pharmaceutical drug, narcotic drug, dialectal past participle of \textit{to drag}), but the meaning and POS of \textit{penicillin} are rather fixed.

To better capture the semantic variability of words, we propose a novel embedding method that produces vectors with stochastic dimensionality.  By employing the same mathematical tools that allow the definition of an \textit{Infinite Restricted Boltzmann Machine} \citep{cote2015infinite}, we describe a log-linear energy-based model--called the \textit{Infinite Skip-Gram} model (iSGM)--that defines a joint distribution over a word vector, a context vector, and their dimensionality.  During training, the iSGM allows word representations to grow naturally based on how well they can predict their context.  This behavior enables the vectors of specific words to have few dimensions and the vectors of vague words to elongate as needed.  Manual and experimental analysis reveals this dynamic representation elegantly captures specificity and polysemy without explicit definition of such concepts within the model, which has been necessary in previous work.

\section{Original Skip-Gram}
The \textit{Skip-Gram} (SG) architecture proposed by \cite{mikolov2013distributed} learns a word's embedding via maximizing the log probability of that word's context (i.e. the words occurring within a fixed-sized window around the input word).  A formal definition is as follows.  Let $\mathbf{w}_{i} \in \mathbb{R}^{d}$ be a $d$-dimensional, real-valued vector representing the $i$th input word $w_{i}$, and let $\mathbf{c}_{k} \in \mathbb{R}^{d}$ be a vector representing the $k$th context word $c_{k}$ appearing in a ($K-1$)-sized window around an instance of $w_{i}$ in some training corpus $\mathcal{D}$.  Training the SG model reduces to minimizing the following objective function: \begin{equation}\begin{split}\label{sg_def} \mathcal{L}_{SG} &= \sum_{i=1}^{|\mathcal{D}|} \sum_{i-K\le k \le i+K, k\ne i} - \log p(c_{k} | w_{i}) \\ &= \sum_{i=1}^{|\mathcal{D}|} \sum_{i-K\le k \le i+K, k\ne i} - \log \frac{e^{\mathbf{c}_{k}^{T}\mathbf{w}_{i}}}{\sum_{v=1}^{V_{c}} e^{\mathbf{c}_{v}^{T}\mathbf{w}_{i}}} \\ &= \sum_{i=1}^{|\mathcal{D}|} \sum_{i-K\le k \le i+K, k\ne i} - \mathbf{c}_{k}^{T}\mathbf{w}_{i} + \log \sum_{v=1}^{V_{c}} e^{\mathbf{c}_{v}^{T}\mathbf{w}_{i}} \end{split}\end{equation} where $V_{c}$ is the size of the context vocabulary.  Stochastic gradient descent is used to update not only $\mathbf{w}_{i}$ but also $\mathbf{c}_{k}$ and $\mathbf{c}_{v}$.  From here forward we drop the summations over the corpus and context window to simplify notation.

Notice the cost of these gradient computations is proportional to $V_{c}$, which is usually several million or more.  A \textit{hierarchical softmax} or \textit{negative sampling} are commonly used to alleviate the computation burden of full normalization \citep{mikolov2013distributed}.  We'll only consider the latter, but all models discussed can be can be adapted straightforwardly to use a hierarchical softmax instead.  Negative sampling consists of randomly sampling (either from the uniform or empirical distribution) words to serve as negative examples--words that \textit{do not} appear in the current context window--and optimizing so that the true context word has a higher likelihood than the samples.  Using negative sampling for computing Equation \ref{sg_def} results in the following modification:  \begin{equation}\begin{split}\label{sg_w_neg} - \mathbf{c}_{k}^{T}\mathbf{w}_{i} + \log \sum_{v=1}^{V_{c}} e^{\mathbf{c}_{v}^{T}\mathbf{w}_{i}} &\approx - \mathbf{c}_{k}^{T}\mathbf{w}_{i} + \log V_{c} \mathbb{E}_{c_{v} \sim p_{\mathcal{D}}(c)} [e^{\mathbf{c}_{v}^{T}\mathbf{w}_{i}}] \\ &= -\mathbf{c}_{k}^{T}\mathbf{w}_{i} + \log V_{c} \frac{1}{S} \sum_{s=1}^{S} e^{\mathbf{c}_{s}^{T}\mathbf{w}_{i}} \end{split}\end{equation} for $S$ negative samples.  Notice that we have defined a slightly different negative sampling SG objective than proposed in \cite{mikolov2013distributed}, which incorporates logistic regression to discriminate the true context word from the samples.  However, other embedding models such as \cite{huang2013learning} have used the negative samples directly in the normalizing sum, as we do.  

\section{Infinite Skip-Gram}
We now propose our novel modification to the Skip-Gram architecture, which we call the \textit{Infinite Skip-Gram} model (iSGM).  Let word vectors $\mathbf{w}_{i} \in \mathbb{R}^{\infty}$ and context vectors $\mathbf{c}_{k} \in \mathbb{R}^{\infty}$ be infinite dimensional (instead of fixed dimensionality $d$), and redefine the model to be the following joint Gibbs distribution over $w_{i}$, $c_{k}$, and a random positive integer $z \in \mathbb{Z}^{+}$ denoting the maximum index over which to compute the vector inner product: \begin{equation}
p(w_{i}, c_{k}, z) = \frac{1}{Z} e^{-E(\mathbf{w}_{i}, \mathbf{c}_{k}, z)} 
\end{equation} where $Z = \sum_{\mathbf{w}} \sum_{\mathbf{c}} \sum_{z} e^{-E(\mathbf{w}, \mathbf{c}, z)}$, also known as the partition function.  Define the energy function as \begin{equation}
E(\mathbf{w}_{i}, \mathbf{c}_{k}, z) = -\sum_{j=1}^{z} w_{i,j}c_{k.j} - \beta_{i} - w_{i,j}^{2} - c_{k,j}^{2}
\end{equation} where $\beta_{i} = \log a$ for $1 < a < \infty \in \mathbb{R}$, representing a per-dimension penalty, which we will later show is necessary to make the infinite sum into a convergent Geometric series.  

\subsection{A Finite Partition Function}
At first glance the iSGM seems intractable since the partition function, containing a sum over all countably infinite values of $z$, would seem to be divergent and thus incomputable.  However, it is not, due to two key properties first proposed by \cite{cote2015infinite} to define a \textit{Restricted Boltzmann Machine} with an infinite number of hidden units (iRBM).  They are: \begin{enumerate}
  \item \textbf{Sparsity penalty}: The $L2$ penalty incorporated into $E(\mathbf{w}_{i}, \mathbf{c}_{k}, z)$ (i.e. the $w_{i,j}^{2}$ and $c_{k,j}^{2}$ terms) ensures the word and context vectors must have a finite two-norm.  That is to say all elements of $\mathbf{w}$ and $\mathbf{c}$ at an index greater than or equal to some sufficiently large index $l$ \emph{must be zero}.  No proper optimization method could converge to the infinite solution \citep{cote2015infinite}.   
  \item \textbf{Per-dimension constant penalty}:  The energy function's $\beta_{i} = \log a$ for $a \in (1, \infty)$ term results in dimensions greater than $l$ becoming a convergent Geometric series.  This is discussed further below and shown in detail in the appendix.  
\end{enumerate}
With those two properties in mind, consider the conditional distribution of $z$ given an input and context word: \begin{equation}
p(z | \mathbf{w}, \mathbf{c}) = \frac{e^{-E(\mathbf{w}, \mathbf{c}, z)}}{\sum_{z'=1}^{\infty} e^{-E(\mathbf{w}, \mathbf{c}, z')}}. \end{equation}  Again, the denominator looks problematic due to the infinite sum, but notice the following (a detailed derivation is in the appendix): \begin{equation}\begin{split}\label{z_finite}
Z_{z} &= \sum_{z'=1}^{\infty} e^{-E(\mathbf{w}, \mathbf{c}, z')}  = \sum_{z'=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z')} + \sum_{z'=l+1}^{\infty} e^{-E(\mathbf{w}, \mathbf{c}, z')}  \\  &= \sum_{z'=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z')} + e^{-E(\mathbf{w}, \mathbf{c}, l)} \sum_{z'=0}^{\infty} e^{  -z' \log a } \\ &= \sum_{z'=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z')} + e^{-E(\mathbf{w}, \mathbf{c}, l)} \sum_{z'=0}^{\infty} \frac{1}{a^{  z' }} \\ &= \sum_{z'=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z')} + \frac{a}{a-1}e^{-E(\mathbf{w}, \mathbf{c}, l)}.
\end{split}
\end{equation}  As stated in other words above, the sparsity penalty allows the sum to be split as it is in step \#2 into a finite term ($\sum_{z'=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z')}$) and an infinite sum ($\sum_{z'=l+1}^{\infty} e^{-E(\mathbf{w}, \mathbf{c}, z')}$) at an index $l$ such that $w_{i,j}=c_{k,j}=0$ $\forall j>l$.  After $e^{-E(\mathbf{w}, \mathbf{c}, l)}$ is factored out of the second term, all remaining $w_{i,j}$ and $c_{k,j}$ terms are zero.  A few steps of algebra then reveal the presence of a convergent Geometric series.  Intuitively, we can think of the second term, $\frac{a}{a-1}e^{-E(\mathbf{w}, \mathbf{c}, l)}$, as quantifying the data's need to expand the model's capacity given $w$ and $c$.   

\subsection{Learning}
We now turn our attention to learning the iSGM.  We will look to treat $z$, the random index variable, as a nuisance parameter and optimize an upper-bound to the traditional Skip-Gram objective:\begin{equation}\begin{split}\label{iSGM_obj}
\mathcal{L}_{SG} &= - \log p(c_{k} | w_{i}) \\ &= - \log \sum_{z=1}^{\infty} p(c_{k}, z| w_{i}) \\ &= - \log \sum_{z=1}^{\infty} p(c_{k}| w_{i}, z) p(z | w_{i}) \\ &\le  \sum_{z=1}^{\infty} p(z | w_{i}) -\log p(c_{k}| w_{i}, z) \\ &= \mathbb{E}_{z|w}[- \log p( c_{k}| w_{i}, z)] = \mathcal{L}_{iSG}. \end{split}\end{equation}  This expectation has, somewhat surprisingly, an analytical form, which can be seen by exploiting the same $L2$-based arguments used to show the partition function to be finite (see the appendix): $$\mathbb{E}_{z|w}[- \log p( c_{k}| w_{i}, z)] = -\sum_{z=1}^{l} p(z | w_{i})  \log p(c_{k}|w_{i},z) + \frac{-a}{a-1} p(z=l | w_{i}) \log p(c_{k} | w_{i}, l). $$  However, we are unable to work with this analytical form directly since $\nabla_{\mathbf{w}} \mathcal{L}_{iSG}$ is infinite dimensional itself.  A work-around would be to manually specify $l$, thus making the gradients for dimensions greater than $l$ zero and hence able to be ignored.  This approach is still undesirable though since we've, for all intents and purposes, regressed to defining a finite dimensional model without the growing behavior that motivates the use of the iSGM.      

Alternatively, we will learn the iSGM via a procedure analogous to the one used for the iRBM \citep{cote2015infinite}.  The expectation in Equation \ref{iSGM_obj} will be computed with a Monte Carlo approximation, i.e. $$\mathbb{E}_{z|w}[- \log p( c_{k}| w_{i}, z)] \approx \frac{1}{M} \sum_{m=1}^{M}  - \log p( c_{k}| w_{i}, \hat z_{m}) $$ for $M$ samples of $\hat z \sim p(z|w_{i})$.  Yet there's still a problem in that $\hat z \in [1, \infty)$ and therefore a very large dimensionality could be sampled, which would incur painful computational costs when performing gradient updates and result in unsatisfactory modeling since it's unlikely the data would need that much capacity (usually a few hundred dimensions are enough for good embeddings).  To remedy this situation, if a $\hat z$ value greater than $l$ is sampled, we will set $\hat z = l + 1$, thus allowing the model to grow only one dimension at a time (just as done for the iRBM).  Constraining growth in this way reduces computational costs as well since $\hat z$ can be drawn from a ($l+1$)-dimensional multinoulli distribution with parameters $$\left \{\theta_{1}=p(z=1|w),\ldots, \theta_{l}=p(z=l|w), \theta_{l+1}=P(z>l|w)=\frac{1}{Z_{z}}\frac{a}{a-1}e^{-E(\mathbf{w}, \mathbf{c}, l)} \right \}$$ where $Z_{z}$ is given in Equation \ref{z_finite}.  Sampling in this way allows for dynamic yet tractable growing behavior.  The intuition is the model can sample a dimension less than or equal to $l$ if $l$ is already sufficiently large or sample the ($l+1$)th option if not, choosing to grow the model's capacity.  Lastly, note that an alternating hybrid strategy is also possible in which we sample for some gradient steps and use the analytical form for others.  \cite{xu2015show} uses a similar strategy for learning a visual attention model.  

\section{Related Work}
This work is motivated by and extends the recent flurry of research into representing words as high-dimensional vectors.  In this section, we'll summarize the \textit{Skip-Gram} (SG) NWE model, which is implemented by the popular software \textit{Word2Vec}.  We'll then discuss extensions to NWEs that have attempted to capture finer concepts such as specificity and polysemy.

\subsection{Polysemy}
Notice the SG architecture has no means of identifying \textit{polysemy} i.e. when a word has multiple meanings.  For instance, the word \textit{bank} can refer to the financial institution or the land alongside a river.  While these senses of \textit{bank} are essentially independent and probably should be treated as two different words entirely, SG has no ability use multiple vectors.  

This problem has been noted and addressed by several previous publications we now review.  One approach is to perform some type of clustering  

\subsection{Term Specificity}

\section{Qualitative Evaluation}

\section{Experimental Evaluation}

\section{Conclusions and Future Work}

\section*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{iclr2016_conference}
\bibliographystyle{iclr2016_conference}

\section*{Appendix}
\subsection{Finite Partition Function}
\begin{equation}\begin{split}\label{z_finite_full}
Z_{z} &= \sum_{z=1}^{\infty} e^{-E(\mathbf{w}, \mathbf{c}, z)} \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + \sum_{z=l+1}^{\infty} e^{-E(\mathbf{w}, \mathbf{c}, z)} \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + \sum_{z=l+1}^{\infty} e^{-E(\mathbf{w}, \mathbf{c}, l) + \sum_{i=l+1}^{z} w_{i}c_{i} - \beta_{i} - w_{i}^{2} - c_{i}^{2}} \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + e^{-E(\mathbf{w}, \mathbf{c}, l)} \sum_{z=l+1}^{\infty} e^{  \sum_{i=l+1}^{z} w_{i}c_{i} - \beta_{i} - w_{i}^{2} - c_{i}^{2}} \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + e^{-E(\mathbf{w}, \mathbf{c}, l)} \sum_{z=l+1}^{\infty} e^{  -\sum_{i=l+1}^{z} \beta_{i} } \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + e^{-E(\mathbf{w}, \mathbf{c}, l)} \sum_{z=l+1}^{\infty} e^{  -(z-l-1) \log a } \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + e^{-E(\mathbf{w}, \mathbf{c}, l)} \sum_{z=0}^{\infty} e^{  -z \log a } \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + e^{-E(\mathbf{w}, \mathbf{c}, l)} \sum_{z=0}^{\infty} \frac{1}{a^{  z }} \\ &= \sum_{z=1}^{l} e^{-E(\mathbf{w}, \mathbf{c}, z)} + .\frac{a}{a-1}e^{-E(\mathbf{w}, \mathbf{c}, l)}
\end{split}
\end{equation} 

\subsection{Finite Expectation}
\begin{equation}\begin{split}\label{expectation_full}
\mathbb{E}_{z|w}[- \log p( c_{k}| w_{i}, z)] &= - \sum_{z=1}^{\infty} p(z | w_{i}) \log p(c_{k}| w_{i}, z) \\ &=  - \sum_{z=1}^{l} p(z | w_{i}) \log p(c_{k}| w_{i}, z) +  - \sum_{z=l+1}^{\infty} p(z | w_{i}) \log p(c_{k}| w_{i}, z) .\end{split}\end{equation}  Expanding the second term we have: \begin{equation}\begin{split}\label{expectation_full2}
\sum_{z=l+1}^{\infty} p(z | w_{i}) \log p(c_{k}| w_{i}, z) &=  \sum_{z=l+1}^{\infty} p(z | w_{i}) [\log e^{E(\mathbf{w}_{i}, \mathbf{c}_{k}, z)} - \log \sum_{c'} e^{E(\mathbf{w}_{i}, \mathbf{c}', z)} ] \\ &= \sum_{z=l+1}^{\infty} p(z | w_{i}) [\log e^{E(\mathbf{w}_{i}, \mathbf{c}_{k}, l)}e^{  -(z-l-1) \log a } - \log \sum_{c'} e^{E(\mathbf{w}_{i}, \mathbf{c}', l)}e^{  -(z-l-1) \log a } ] \\ &= \sum_{z=l+1}^{\infty} p(z | w_{i}) [\log \frac{1}{a^{z-l-1}}e^{E(\mathbf{w}_{i}, \mathbf{c}_{k}, l)} - \log \frac{1}{a^{z-l-1}}\sum_{c'} e^{E(\mathbf{w}_{i}, \mathbf{c}', l)}] \\ &= \log p(c_{k}|w_{i},l)\sum_{z=l+1}^{\infty} p(z | w_{i}) \\ &= \log p(c_{k}|w_{i},l)\sum_{z=l+1}^{\infty} p(z | w_{i}) \end{split}\end{equation} 


\subsection{Training}
\subsubsection{Setup}
Consider the set of examples $\{(w_i, c_1), \ldots, (w_i, c_D)\}$ where $d-1$ of the contexts $c_d$ are negative
and random and one context $c_{k}$ is positive and occurs in text with $w_i$.

Two probabilities we need are
\begin{align*}
P(\mathbf{c}_k \, | \, \mathbf{w}_i, z) &= \frac{P(\mathbf{c}_k, \mathbf{w}_i, z)}{P(\mathbf{w}_i, z)} \\
&= \frac{P(\mathbf{c}_k, \mathbf{w}_i, z)}{\sum_{v=1}^{V_c} P(\mathbf{c}_v, \mathbf{w}_i, z)} \\
&= \frac{e^{-E(\mathbf{c}_k, \mathbf{w}_i, z)}}{\sum_{v=1}^{V_c} e^{-E(\mathbf{c}_v, \mathbf{w}_i, z)}} \\
&\approx \frac{e^{-E(\mathbf{c}_k, \mathbf{w}_i, z)}}{\sum_{d=1}^{D} e^{-E(\mathbf{c}_d, \mathbf{w}_i, z)}} \\
\end{align*}

and assuming $c_1, \ldots, c_P$ are part of the occurring context window,
\begin{align*}
p(z \, | \, \mathbf{w}_i) &= \frac{p(z, \mathbf{w}_i)}{p(\mathbf{w}_i)} \\
&= \frac{\sum_{v=1}^{V_c} p(\mathbf{c}_v, \mathbf{w}_i, z)}
{\sum_{t=1}^{l} \sum_{v=1}^{V_c} p(\mathbf{c}_v, \mathbf{w}_i, z_t)} \\
&= \frac{\sum_{v=1}^{V_c} e^{-E(\mathbf{c}_v, \mathbf{w}_i, z)}}
{\sum_{t=1}^{l} \sum_{v=1}^{V_c} e^{-E(\mathbf{c}_v, \mathbf{w}_i, z_t)}} \\
&\approx \frac{\sum_{p=1}^{P} e^{-E(\mathbf{c}_p, \mathbf{w}_i, z)}} 
{\sum_{t=1}^{l} \sum_{p=1}^{P} e^{-E(\mathbf{c}_p, \mathbf{w}_i, z_t)}}
\end{align*}

So, rather than sum over the entire context, we are summing over the context
around the word $\mathbf{w}_i$.

From the \emph{Show, Attend, and Tell} paper, we have that $L_{s}$, an upper bound on 
true loss \\
$L = \mathbb{E}_{z \, | \, \mathbf{w}} [-\log p(\mathbf{c}_k \, | \, \mathbf{w}_i, z)]$, is
$$
L_{s} = -\sum_{z=1}^{l} p(z \, | \, \mathbf{w}_i) \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , z)
$$

Optimizing for parameters $W$ we have,
$$
\frac{\partial L_{s}}{\partial W} = -\sum_{z=1}^{l} p(z \, | \, \mathbf{w}_i)  
\left[
\frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , z)}{\partial W} +
\log p(\mathbf{c}_k \, | \, \mathbf{w}_i , z)
\frac{\partial \log  p(z \, | \, \mathbf{w}_i)}{\partial W}
\right]
$$

As in \emph{Show, Attend, and Tell}, we calculate this gradient approximately by sampling
$\tilde{z}_m \sim p(z \, | \, \mathbf{w}_i)$, and calculating
$$
\frac{\partial L_{s}}{\partial W} \approx - \frac{1}{M} \sum_{m=1}^{M} 
\left[
\frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial W} +
\log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)
\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}{\partial W}
\right]
$$

\subsubsection{Gradient calculation}
We first calculate $\frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial W}$. 

We have that 
\begin{align*}
 \frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial -E(\mathbf{c}_d, \mathbf{w}_i, z)} &= 
\frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}
\frac{\partial p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial -E(\mathbf{c}_d, \mathbf{w}_i, z)} \\
&= (\mathbbm{1}_{d = k} - p(\mathbf{c}_d \, | \, \mathbf{w}_i , \tilde{z}_m)) & \text{(By Derivative of Softmax)}
\end{align*}

Then,
\begin{align*}
\frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial \mathbf{w}_{ij}} &=
\sum_{d=1}^{D} \frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial -E(\mathbf{c}_d, \mathbf{w}_i, z)}
\frac{\partial -E(\mathbf{c}_d, \mathbf{w}_i, z)}{\partial \mathbf{w}_{ij}} \\
&= \sum_{d=1}^{D} (\mathbbm{1}_{d = k} - p(\mathbf{c}_d \, | \, \mathbf{w}_i , \tilde{z}_m) (\mathbf{c}_{dj} - 2 \lambda \mathbf{w}_{ij})
\end{align*}

And,
\begin{align*}
\frac{\partial \log p(\mathbf{c}_k \, | \, \mathbf{w}_i , \tilde{z}_m)}{\partial \mathbf{c}_{dj}} &=
(\mathbbm{1}_{d = k} - p(\mathbf{c}_d \, | \, \mathbf{w}_i , \tilde{z}_m) (\mathbf{w}_{ij} - 2 \lambda \mathbf{c}_{dj})
\end{align*}

We now calculate $\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}{\partial W}$.

We have that 
\begin{align*}
\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}
{\partial \sum_{p=1}^{P} -E(\mathbf{c}_p, \mathbf{w}_i, z_t)} 
&=
\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}
{\partial  p(\tilde{z}_m \, | \, \mathbf{w}_i)}
\frac{\partial  p(\tilde{z}_m \, | \, \mathbf{w}_i)}
{\partial \sum_{p=1}^{P} -E(\mathbf{c}_p, \mathbf{w}_i, z_t)} \\
&= 
\left( \mathbbm{1}_{z_t = \tilde{z}_m} - \sum_{p=1}^{P} p(\mathbf{c}_p, \mathbf{w}_i, z_t) \right) \\
\end{align*}

Then,
\begin{align*}
\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}{\partial \mathbf{w}_{ij}} 
&=
\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}
{\partial \sum_{p=1}^{P} -E(\mathbf{c}_p, \mathbf{w}_i, z_t)}
\frac{\partial \sum_{p=1}^{P} -E(\mathbf{c}_p, \mathbf{w}_i, z_t)}{\partial \mathbf{w}_{ij}} \\
&=
\left( \mathbbm{1}_{z_t = \tilde{z}_m} - \sum_{p=1}^{P} p(\mathbf{c}_p, \mathbf{w}_i, z_t) \right)
\sum_{p=1}^{P} \left( \mathbf{c}_{pj} - 2 \lambda \mathbf{w}_{ij} \right)
\end{align*}

And,
\begin{align*}
\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}{\partial \mathbf{c}_{pj}} 
&= 
\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}
{\partial \sum_{p=1}^{P} -E(\mathbf{c}_p, \mathbf{w}_i, z_t)}
\frac{\partial \sum_{p=1}^{P} -E(\mathbf{c}_p, \mathbf{w}_i, z_t)}{\partial \mathbf{c}_{pj}} \\
&= 
\left( \mathbbm{1}_{z_t = \tilde{z}_m} - \sum_{p=1}^{P} p(\mathbf{c}_p, \mathbf{w}_i, z_t) \right)
\left( \mathbf{w}_{ij} - 2 \lambda \mathbf{c}_{pj} \right)
\end{align*}

Notice that $\frac{\partial \log  p(\tilde{z}_m \, | \, \mathbf{w}_i)}{\partial \mathbf{c}_{pj}} $ 
is only for the positive contexts that occur in a certain window. This makes sense that
negative context words don't contribute to this gradient.

\end{document}
